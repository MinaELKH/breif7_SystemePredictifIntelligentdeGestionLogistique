{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1971ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ff7c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataCo - Prédiction Retards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.default.parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark initialisé ✔️\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m df_test \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCo - Prédiction Retards\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark initialisé ✔️\")\n",
    "\n",
    "df_test = spark.range(10)\n",
    "df_test.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\",\n",
    "    \"Benefit per order\",\n",
    "    \"Sales per customer\",\n",
    "    \"Order Item Discount\",\n",
    "    \"Order Item Discount Rate\",\n",
    "    \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\",\n",
    "    \"Order Item Quantity\",\n",
    "    \"Sales\",\n",
    "    \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\",\n",
    "    \"Shipping Mode\",\n",
    "    \"Market\",\n",
    "    \"Customer Segment\",\n",
    "    \"Order State\",\n",
    "    \"Order Region\",\n",
    "    \"Category Name\"\n",
    "]\n",
    "\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]  # Variable cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35366639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['Days for shipment (scheduled), 'Benefit per order, 'Sales per customer, 'Order Item Discount, 'Order Item Discount Rate, 'Order Item Product Price, 'Order Item Profit Ratio, 'Order Item Quantity, 'Sales, 'Order Profit Per Order, 'Type, 'Shipping Mode, 'Market, 'Customer Segment, 'Order State, 'Order Region, 'Category Name, 'Late_delivery_risk]\n",
      "+- Filter NOT (Delivery Status#29 = Shipping canceled)\n",
      "   +- Relation [Type#24,Days for shipping (real)#25,Days for shipment (scheduled)#26,Benefit per order#27,Sales per customer#28,Delivery Status#29,Late_delivery_risk#30,Category Id#31,Category Name#32,Customer City#33,Customer Country#34,Customer Email#35,Customer Fname#36,Customer Id#37,Customer Lname#38,Customer Password#39,Customer Segment#40,Customer State#41,Customer Street#42,Customer Zipcode#43,Department Id#44,Department Name#45,Latitude#46,Longitude#47,... 29 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Days for shipment (scheduled): int, Benefit per order: double, Sales per customer: double, Order Item Discount: double, Order Item Discount Rate: double, Order Item Product Price: double, Order Item Profit Ratio: double, Order Item Quantity: int, Sales: double, Order Profit Per Order: double, Type: string, Shipping Mode: string, Market: string, Customer Segment: string, Order State: string, Order Region: string, Category Name: string, Late_delivery_risk: int\n",
      "Project [Days for shipment (scheduled)#26, Benefit per order#27, Sales per customer#28, Order Item Discount#55, Order Item Discount Rate#56, Order Item Product Price#58, Order Item Profit Ratio#59, Order Item Quantity#60, Sales#61, Order Profit Per Order#63, Type#24, Shipping Mode#76, Market#48, Customer Segment#40, Order State#65, Order Region#64, Category Name#32, Late_delivery_risk#30]\n",
      "+- Filter NOT (Delivery Status#29 = Shipping canceled)\n",
      "   +- Relation [Type#24,Days for shipping (real)#25,Days for shipment (scheduled)#26,Benefit per order#27,Sales per customer#28,Delivery Status#29,Late_delivery_risk#30,Category Id#31,Category Name#32,Customer City#33,Customer Country#34,Customer Email#35,Customer Fname#36,Customer Id#37,Customer Lname#38,Customer Password#39,Customer Segment#40,Customer State#41,Customer Street#42,Customer Zipcode#43,Department Id#44,Department Name#45,Latitude#46,Longitude#47,... 29 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [Days for shipment (scheduled)#26, Benefit per order#27, Sales per customer#28, Order Item Discount#55, Order Item Discount Rate#56, Order Item Product Price#58, Order Item Profit Ratio#59, Order Item Quantity#60, Sales#61, Order Profit Per Order#63, Type#24, Shipping Mode#76, Market#48, Customer Segment#40, Order State#65, Order Region#64, Category Name#32, Late_delivery_risk#30]\n",
      "+- Filter (isnotnull(Delivery Status#29) AND NOT (Delivery Status#29 = Shipping canceled))\n",
      "   +- Relation [Type#24,Days for shipping (real)#25,Days for shipment (scheduled)#26,Benefit per order#27,Sales per customer#28,Delivery Status#29,Late_delivery_risk#30,Category Id#31,Category Name#32,Customer City#33,Customer Country#34,Customer Email#35,Customer Fname#36,Customer Id#37,Customer Lname#38,Customer Password#39,Customer Segment#40,Customer State#41,Customer Street#42,Customer Zipcode#43,Department Id#44,Department Name#45,Latitude#46,Longitude#47,... 29 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Days for shipment (scheduled)#26, Benefit per order#27, Sales per customer#28, Order Item Discount#55, Order Item Discount Rate#56, Order Item Product Price#58, Order Item Profit Ratio#59, Order Item Quantity#60, Sales#61, Order Profit Per Order#63, Type#24, Shipping Mode#76, Market#48, Customer Segment#40, Order State#65, Order Region#64, Category Name#32, Late_delivery_risk#30]\n",
      "+- *(1) Filter (isnotnull(Delivery Status#29) AND NOT (Delivery Status#29 = Shipping canceled))\n",
      "   +- FileScan csv [Type#24,Days for shipment (scheduled)#26,Benefit per order#27,Sales per customer#28,Delivery Status#29,Late_delivery_risk#30,Category Name#32,Customer Segment#40,Market#48,Order Item Discount#55,Order Item Discount Rate#56,Order Item Product Price#58,Order Item Profit Ratio#59,Order Item Quantity#60,Sales#61,Order Profit Per Order#63,Order Region#64,Order State#65,Shipping Mode#76] Batched: false, DataFilters: [isnotnull(Delivery Status#29), NOT (Delivery Status#29 = Shipping canceled)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/data/DataCoSupplyChainDataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Delivery Status), Not(EqualTo(Delivery Status,Shipping canceled))], ReadSchema: struct<Type:string,Days for shipment (scheduled):int,Benefit per order:double,Sales per customer:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "        .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "        .select(*base_cols)\n",
    ")\n",
    "\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2621d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Days for shipment (scheduled): integer (nullable = true)\n",
      " |-- Benefit per order: double (nullable = true)\n",
      " |-- Sales per customer: double (nullable = true)\n",
      " |-- Order Item Discount: double (nullable = true)\n",
      " |-- Order Item Discount Rate: double (nullable = true)\n",
      " |-- Order Item Product Price: double (nullable = true)\n",
      " |-- Order Item Profit Ratio: double (nullable = true)\n",
      " |-- Order Item Quantity: integer (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Order Profit Per Order: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Shipping Mode: string (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Customer Segment: string (nullable = true)\n",
      " |-- Order State: string (nullable = true)\n",
      " |-- Order Region: string (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Late_delivery_risk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34778d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|Late_delivery_risk|count|\n",
      "+------------------+-----+\n",
      "|                 1|98977|\n",
      "|                 0|73788|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_verifier_equilibre = df.groupBy(\"Late_delivery_risk\").count()\n",
    "df_verifier_equilibre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4e2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vérification des valeurs manquantes ===\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "|Days for shipment (scheduled)|Benefit per order|Sales per customer|Order Item Discount|Order Item Discount Rate|Order Item Product Price|Order Item Profit Ratio|Order Item Quantity|Sales|Order Profit Per Order|Type|Shipping Mode|Market|Customer Segment|Order State|Order Region|Category Name|Late_delivery_risk|\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "|                            0|                0|                 0|                  0|                       0|                       0|                      0|                  0|    0|                     0|   0|            0|     0|               0|          0|           0|            0|                 0|\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "\n",
      "Lignes avant: 172765, Lignes après: 172765\n",
      "\n",
      "=== Encodage des variables catégorielles ===\n",
      "\n",
      "=== Normalisation des features numériques ===\n",
      "\n",
      "=== Assembly final ===\n",
      "\n",
      "=== Dataset prêt pour le modèle ===\n",
      "Nombre total de lignes: 172765\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                           |label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.7768282727817317,0.6632881255885652,1.0943234421372077,-0.3461632773000043,-0.874991396761714,1.33324370715565,0.3626133523238609,-0.7757599467342629,0.9360159184329607,0.6632881255885652,0.0,0.0,2.0,0.0,45.0,5.0,36.0]      |0    |\n",
      "|[0.7768282727817317,-2.5980694911208535,1.0670221427380402,-0.1958328458006766,-0.7330047632371504,1.33324370715565,-1.973383754277715,-0.7757599467342629,0.9360159184329607,-2.5980694911208535,1.0,0.0,2.0,0.0,133.0,9.0,36.0]  |1    |\n",
      "|[0.7768282727817317,-2.5855162491541663,1.053371746072637,-0.1206675642814435,-0.5910181865072288,1.33324370715565,-1.973383754277715,-0.7757599467342629,0.9360159184329607,-2.5855162491541663,3.0,0.0,2.0,0.0,133.0,9.0,36.0]   |0    |\n",
      "|[0.7768282727817317,0.007930989071698624,1.012503366506201,0.1043697696724924,-0.44903156718132564,1.33324370715565,-0.08744112159368994,-0.7757599467342629,0.9360159184329607,0.007930989071698624,0.0,0.0,2.0,2.0,10.0,3.0,36.0]|0    |\n",
      "|[0.7768282727817317,1.0749586509106361,0.9579012737762274,0.40503063312947235,-0.16505830013219855,1.33324370715565,0.7055120003428235,-0.7757599467342629,0.9360159184329607,1.0749586509106361,2.0,0.0,2.0,1.0,10.0,3.0,36.0]    |0    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Distribution de la variable cible ===\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|98977|\n",
      "|    0|73788|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# VOS FEATURES\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order State\", \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]\n",
    "\n",
    "# Charger les données\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*base_cols)\n",
    ")\n",
    "\n",
    "\n",
    "# 1. GESTION DES VALEURS MANQUANTES\n",
    "print(\"=== Vérification des valeurs manquantes ===\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "df_clean = df.dropna()\n",
    "print(f\"Lignes avant: {df.count()}, Lignes après: {df_clean.count()}\")\n",
    "\n",
    "\n",
    "# 2. ENCODAGE DES VARIABLES CATÉGORIELLES\n",
    "print(\"\\n=== Encodage des variables catégorielles ===\")\n",
    "indexers = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name, \n",
    "        outputCol=col_name + \"_indexed\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "pipeline_encoding = Pipeline(stages=indexers)\n",
    "model_encoding = pipeline_encoding.fit(df_clean)\n",
    "df_encoded = model_encoding.transform(df_clean)\n",
    "\n",
    "encoded_cols = [c + \"_indexed\" for c in feature_categorical_cols]\n",
    "\n",
    "\n",
    "# 3. NORMALISATION DES FEATURES NUMÉRIQUES\n",
    "print(\"\\n=== Normalisation des features numériques ===\")\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=feature_numeric_cols, \n",
    "    outputCol=\"numeric_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "df_assembled = assembler_num.transform(df_encoded)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\", \n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True, \n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "\n",
    "# 4. PRÉPARATION FINALE POUR LE MODÈLE\n",
    "print(\"\\n=== Assembly final ===\")\n",
    "all_feature_cols = [\"numeric_features_scaled\"] + encoded_cols\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=all_feature_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "df_final = assembler_final.transform(df_scaled)\n",
    "\n",
    "# Dataset prêt\n",
    "df_ready = df_final.select(\"features\", col(\"Late_delivery_risk\").alias(\"label\"))\n",
    "\n",
    "print(\"\\n=== Dataset prêt pour le modèle ===\")\n",
    "print(f\"Nombre total de lignes: {df_ready.count()}\")\n",
    "df_ready.show(5, truncate=False)\n",
    "\n",
    "# Distribution finale\n",
    "print(\"\\n=== Distribution de la variable cible ===\")\n",
    "df_ready.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ed009",
   "metadata": {},
   "source": [
    "## Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9455f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f549a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.751825774386722\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import  GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order State\", \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*(feature_numeric_cols + feature_categorical_cols + [\"Late_delivery_risk\"]))\n",
    ").dropna()\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_indexed\", handleInvalid=\"keep\")\n",
    "            for c in feature_categorical_cols]\n",
    "\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=feature_numeric_cols,\n",
    "    outputCol=\"num_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"num_features\",\n",
    "    outputCol=\"num_features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=[\"num_features_scaled\"] + [c+\"_indexed\" for c in feature_categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxBins=2000\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler_num, scaler, assembler_final, rf])\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "pred = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "print(\"AUC =\", evaluator.evaluate(pred))\n",
    "\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5,\n",
    "    maxBins=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7c08e-b6e9-4dbb-88df-ca9b8b96fc34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Charger les données\u001b[39;00m\n\u001b[1;32m     22\u001b[0m base_cols \u001b[38;5;241m=\u001b[39m feature_numeric_cols \u001b[38;5;241m+\u001b[39m feature_categorical_cols \u001b[38;5;241m+\u001b[39m cible\n\u001b[1;32m     24\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/DataCoSupplyChainDataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelivery Status\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShipping canceled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39mbase_cols)\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Nettoyer les données\u001b[39;00m\n\u001b[1;32m     34\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# VOS FEATURES\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order State\", \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]\n",
    "\n",
    "# Charger les données\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*base_cols)\n",
    ")\n",
    "\n",
    "# Nettoyer les données\n",
    "df_clean = df.dropna()\n",
    "print(f\"Dataset nettoyé: {df_clean.count()} lignes\")\n",
    "\n",
    "# Vérifier le nombre de valeurs uniques par colonne catégorielle\n",
    "print(\"\\n=== Nombre de valeurs uniques par variable catégorielle ===\")\n",
    "for col_name in feature_categorical_cols:\n",
    "    nb_unique = df_clean.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {nb_unique} valeurs uniques\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# ÉTAPES DE PRÉTRAITEMENT\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Encodage des variables catégorielles avec StringIndexer\n",
    "indexers = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name, \n",
    "        outputCol=col_name + \"_indexed\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "indexed_cols = [c + \"_indexed\" for c in feature_categorical_cols]\n",
    "\n",
    "# 2. OneHotEncoding pour les modèles qui en ont besoin (Neural Network)\n",
    "encoders = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=col_name + \"_indexed\",\n",
    "        outputCol=col_name + \"_encoded\"\n",
    "    )\n",
    "    encoders.append(encoder)\n",
    "\n",
    "encoded_cols = [c + \"_encoded\" for c in feature_categorical_cols]\n",
    "\n",
    "# 3. Assembly des features numériques\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=feature_numeric_cols, \n",
    "    outputCol=\"numeric_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4. Normalisation\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\", \n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True, \n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# 5. Assembly final pour Random Forest et GBT (avec indexed)\n",
    "assembler_final_trees = VectorAssembler(\n",
    "    inputCols=[\"numeric_features_scaled\"] + indexed_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 6. Assembly final pour Neural Network (avec OneHot)\n",
    "assembler_final_mlp = VectorAssembler(\n",
    "    inputCols=[\"numeric_features_scaled\"] + encoded_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# MODÈLES NON-LINÉAIRES\n",
    "# ====================================================================\n",
    "\n",
    "# MODÈLE 1 : RANDOM FOREST\n",
    "# SOLUTION : Augmenter maxBins pour gérer plus de catégories\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    maxBins=2000,  # ← AUGMENTÉ pour gérer Order State (1081 valeurs)\n",
    "    minInstancesPerNode=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# MODÈLE 2 : GRADIENT BOOSTING TREES\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    maxBins=2000,  # ← AUGMENTÉ également\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# MODÈLE 3 : NEURAL NETWORK\n",
    "# Pour MLP, on a besoin de compter les features après OneHotEncoding\n",
    "# On va le calculer dynamiquement après le preprocessing\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxIter=100,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# SPLIT TRAIN/TEST\n",
    "# ====================================================================\n",
    "\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nDonnées d'entraînement: {train_data.count()} lignes\")\n",
    "print(f\"Données de test: {test_data.count()} lignes\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# FONCTION POUR ENTRAÎNER ET ÉVALUER UN MODÈLE\n",
    "# ====================================================================\n",
    "\n",
    "def entrainer_et_evaluer(model, nom_modele, use_onehot=False):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODÈLE : {nom_modele}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Choisir le bon assembler\n",
    "    if use_onehot:\n",
    "        assembler_final = assembler_final_mlp\n",
    "        stages = [*indexers, *encoders, assembler_num, scaler, assembler_final, model]\n",
    "    else:\n",
    "        assembler_final = assembler_final_trees\n",
    "        stages = [*indexers, assembler_num, scaler, assembler_final, model]\n",
    "    \n",
    "    # Créer le pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Entraîner\n",
    "    print(f\"⏳ Entraînement en cours...\")\n",
    "    model_trained = pipeline.fit(train_data)\n",
    "    print(f\"✅ Entraînement terminé !\")\n",
    "    \n",
    "    # Prédictions\n",
    "    predictions_train = model_trained.transform(train_data)\n",
    "    predictions_test = model_trained.transform(test_data)\n",
    "    \n",
    "    # Evaluateurs\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    \n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    \n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    auc_train = evaluator_auc.evaluate(predictions_train)\n",
    "    auc_test = evaluator_auc.evaluate(predictions_test)\n",
    "    \n",
    "    accuracy_train = evaluator_accuracy.evaluate(predictions_train)\n",
    "    accuracy_test = evaluator_accuracy.evaluate(predictions_test)\n",
    "    \n",
    "    f1_train = evaluator_f1.evaluate(predictions_train)\n",
    "    f1_test = evaluator_f1.evaluate(predictions_test)\n",
    "    \n",
    "    precision_test = evaluator_precision.evaluate(predictions_test)\n",
    "    recall_test = evaluator_recall.evaluate(predictions_test)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"\\n📊 RÉSULTATS - {nom_modele}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"TRAIN | AUC: {auc_train:.4f} | Accuracy: {accuracy_train:.4f} | F1: {f1_train:.4f}\")\n",
    "    print(f\"TEST  | AUC: {auc_test:.4f} | Accuracy: {accuracy_test:.4f} | F1: {f1_test:.4f}\")\n",
    "    print(f\"TEST  | Precision: {precision_test:.4f} | Recall: {recall_test:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    print(f\"\\n📈 Matrice de confusion - {nom_modele}\")\n",
    "    confusion_matrix = predictions_test.groupBy(\"Late_delivery_risk\", \"prediction\").count()\n",
    "    confusion_matrix.orderBy(\"Late_delivery_risk\", \"prediction\").show()\n",
    "    \n",
    "    return {\n",
    "        'modele': nom_modele,\n",
    "        'auc_test': auc_test,\n",
    "        'accuracy_test': accuracy_test,\n",
    "        'f1_test': f1_test,\n",
    "        'precision_test': precision_test,\n",
    "        'recall_test': recall_test,\n",
    "        'pipeline': model_trained\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# ENTRAÎNER ET COMPARER TOUS LES MODÈLES NON-LINÉAIRES\n",
    "# ====================================================================\n",
    "\n",
    "resultats = []\n",
    "\n",
    "# Random Forest\n",
    "# resultats.append(entrainer_et_evaluer(rf, \"Random Forest\", use_onehot=False))\n",
    "\n",
    "# Gradient Boosting\n",
    "resultats.append(entrainer_et_evaluer(gbt, \"Gradient Boosting Trees\", use_onehot=False))\n",
    "\n",
    "# Neural Network (avec OneHot encoding)\n",
    "# D'abord, configurer les layers correctement\n",
    "# On va le faire après avoir compté les features\n",
    "print(\"\\n⏳ Configuration du Neural Network...\")\n",
    "\n",
    "# Créer un pipeline temporaire pour compter les features\n",
    "temp_pipeline = Pipeline(stages=[*indexers, *encoders, assembler_num, scaler, assembler_final_mlp])\n",
    "temp_model = temp_pipeline.fit(train_data.limit(100))  # Juste pour compter\n",
    "temp_data = temp_model.transform(train_data.limit(1))\n",
    "nb_features = temp_data.select(\"features\").first()[0].size\n",
    "\n",
    "print(f\"Nombre de features après preprocessing: {nb_features}\")\n",
    "\n",
    "# Configurer le MLP avec le bon nombre de features\n",
    "layers = [nb_features, 128, 64, 2]  # 2 classes\n",
    "mlp.setLayers(layers)\n",
    "\n",
    "resultats.append(entrainer_et_evaluer(mlp, \"Neural Network (MLP)\", use_onehot=True))\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# COMPARAISON FINALE\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARAISON DES MODÈLES NON-LINÉAIRES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Modèle':<30} {'AUC':<10} {'Accuracy':<10} {'F1-Score':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for r in resultats:\n",
    "    print(f\"{r['modele']:<30} {r['auc_test']:<10.4f} {r['accuracy_test']:<10.4f} {r['f1_test']:<10.4f} {r['precision_test']:<10.4f} {r['recall_test']:<10.4f}\")\n",
    "\n",
    "# Trouver le meilleur modèle\n",
    "meilleur = max(resultats, key=lambda x: x['f1_test'])\n",
    "print(f\"\\n🏆 MEILLEUR MODÈLE : {meilleur['modele']} (F1-Score: {meilleur['f1_test']:.4f})\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
