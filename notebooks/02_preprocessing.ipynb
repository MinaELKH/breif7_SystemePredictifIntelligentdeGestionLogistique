{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1971ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ff7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCo - Prédiction Retards\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c57f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\",\n",
    "    \"Benefit per order\",\n",
    "    \"Sales per customer\",\n",
    "    \"Order Item Discount\",\n",
    "    \"Order Item Discount Rate\",\n",
    "    \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\",\n",
    "    \"Order Item Quantity\",\n",
    "    \"Sales\",\n",
    "    \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\",\n",
    "    \"Shipping Mode\",\n",
    "    \"Market\",\n",
    "    \"Customer Segment\",\n",
    "    \"Order State\",\n",
    "    \"Order Region\",\n",
    "    \"Category Name\"\n",
    "]\n",
    "\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]  # Variable cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35366639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['Days for shipment (scheduled), 'Benefit per order, 'Sales per customer, 'Order Item Discount, 'Order Item Discount Rate, 'Order Item Product Price, 'Order Item Profit Ratio, 'Order Item Quantity, 'Sales, 'Order Profit Per Order, 'Type, 'Shipping Mode, 'Market, 'Customer Segment, 'Order State, 'Order Region, 'Category Name, 'Late_delivery_risk]\n",
      "+- Filter NOT (Delivery Status#22 = Shipping canceled)\n",
      "   +- Relation [Type#17,Days for shipping (real)#18,Days for shipment (scheduled)#19,Benefit per order#20,Sales per customer#21,Delivery Status#22,Late_delivery_risk#23,Category Id#24,Category Name#25,Customer City#26,Customer Country#27,Customer Email#28,Customer Fname#29,Customer Id#30,Customer Lname#31,Customer Password#32,Customer Segment#33,Customer State#34,Customer Street#35,Customer Zipcode#36,Department Id#37,Department Name#38,Latitude#39,Longitude#40,Market#41,... 28 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Days for shipment (scheduled): int, Benefit per order: double, Sales per customer: double, Order Item Discount: double, Order Item Discount Rate: double, Order Item Product Price: double, Order Item Profit Ratio: double, Order Item Quantity: int, Sales: double, Order Profit Per Order: double, Type: string, Shipping Mode: string, Market: string, Customer Segment: string, Order State: string, Order Region: string, Category Name: string, Late_delivery_risk: int\n",
      "Project [Days for shipment (scheduled)#19, Benefit per order#20, Sales per customer#21, Order Item Discount#48, Order Item Discount Rate#49, Order Item Product Price#51, Order Item Profit Ratio#52, Order Item Quantity#53, Sales#54, Order Profit Per Order#56, Type#17, Shipping Mode#69, Market#41, Customer Segment#33, Order State#58, Order Region#57, Category Name#25, Late_delivery_risk#23]\n",
      "+- Filter NOT (Delivery Status#22 = Shipping canceled)\n",
      "   +- Relation [Type#17,Days for shipping (real)#18,Days for shipment (scheduled)#19,Benefit per order#20,Sales per customer#21,Delivery Status#22,Late_delivery_risk#23,Category Id#24,Category Name#25,Customer City#26,Customer Country#27,Customer Email#28,Customer Fname#29,Customer Id#30,Customer Lname#31,Customer Password#32,Customer Segment#33,Customer State#34,Customer Street#35,Customer Zipcode#36,Department Id#37,Department Name#38,Latitude#39,Longitude#40,Market#41,... 28 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [Days for shipment (scheduled)#19, Benefit per order#20, Sales per customer#21, Order Item Discount#48, Order Item Discount Rate#49, Order Item Product Price#51, Order Item Profit Ratio#52, Order Item Quantity#53, Sales#54, Order Profit Per Order#56, Type#17, Shipping Mode#69, Market#41, Customer Segment#33, Order State#58, Order Region#57, Category Name#25, Late_delivery_risk#23]\n",
      "+- Filter (isnotnull(Delivery Status#22) AND NOT (Delivery Status#22 = Shipping canceled))\n",
      "   +- Relation [Type#17,Days for shipping (real)#18,Days for shipment (scheduled)#19,Benefit per order#20,Sales per customer#21,Delivery Status#22,Late_delivery_risk#23,Category Id#24,Category Name#25,Customer City#26,Customer Country#27,Customer Email#28,Customer Fname#29,Customer Id#30,Customer Lname#31,Customer Password#32,Customer Segment#33,Customer State#34,Customer Street#35,Customer Zipcode#36,Department Id#37,Department Name#38,Latitude#39,Longitude#40,Market#41,... 28 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Days for shipment (scheduled)#19, Benefit per order#20, Sales per customer#21, Order Item Discount#48, Order Item Discount Rate#49, Order Item Product Price#51, Order Item Profit Ratio#52, Order Item Quantity#53, Sales#54, Order Profit Per Order#56, Type#17, Shipping Mode#69, Market#41, Customer Segment#33, Order State#58, Order Region#57, Category Name#25, Late_delivery_risk#23]\n",
      "+- *(1) Filter (isnotnull(Delivery Status#22) AND NOT (Delivery Status#22 = Shipping canceled))\n",
      "   +- FileScan csv [Type#17,Days for shipment (scheduled)#19,Benefit per order#20,Sales per customer#21,Delivery Status#22,Late_delivery_risk#23,Category Name#25,Customer Segment#33,Market#41,Order Item Discount#48,Order Item Discount Rate#49,Order Item Product Price#51,Order Item Profit Ratio#52,Order Item Quantity#53,Sales#54,Order Profit Per Order#56,Order Region#57,Order State#58,Shipping Mode#69] Batched: false, DataFilters: [isnotnull(Delivery Status#22), NOT (Delivery Status#22 = Shipping canceled)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/elkho/OneDrive/Desktop/IA/Briefs/breif7_Système Prédict..., PartitionFilters: [], PushedFilters: [IsNotNull(Delivery Status), Not(EqualTo(Delivery Status,Shipping canceled))], ReadSchema: struct<Type:string,Days for shipment (scheduled):int,Benefit per order:double,Sales per customer:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "        .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "        .select(*base_cols)\n",
    ")\n",
    "\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2621d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Days for shipment (scheduled): integer (nullable = true)\n",
      " |-- Benefit per order: double (nullable = true)\n",
      " |-- Sales per customer: double (nullable = true)\n",
      " |-- Order Item Discount: double (nullable = true)\n",
      " |-- Order Item Discount Rate: double (nullable = true)\n",
      " |-- Order Item Product Price: double (nullable = true)\n",
      " |-- Order Item Profit Ratio: double (nullable = true)\n",
      " |-- Order Item Quantity: integer (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Order Profit Per Order: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Shipping Mode: string (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Customer Segment: string (nullable = true)\n",
      " |-- Order State: string (nullable = true)\n",
      " |-- Order Region: string (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Late_delivery_risk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34778d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|Late_delivery_risk|count|\n",
      "+------------------+-----+\n",
      "|                 1|98977|\n",
      "|                 0|73788|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_verifier_equilibre = df.groupBy(\"Late_delivery_risk\").count()\n",
    "df_verifier_equilibre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4e2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vérification des valeurs manquantes ===\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "|Days for shipment (scheduled)|Benefit per order|Sales per customer|Order Item Discount|Order Item Discount Rate|Order Item Product Price|Order Item Profit Ratio|Order Item Quantity|Sales|Order Profit Per Order|Type|Shipping Mode|Market|Customer Segment|Order State|Order Region|Category Name|Late_delivery_risk|\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "|                            0|                0|                 0|                  0|                       0|                       0|                      0|                  0|    0|                     0|   0|            0|     0|               0|          0|           0|            0|                 0|\n",
      "+-----------------------------+-----------------+------------------+-------------------+------------------------+------------------------+-----------------------+-------------------+-----+----------------------+----+-------------+------+----------------+-----------+------------+-------------+------------------+\n",
      "\n",
      "Lignes avant: 172765, Lignes après: 172765\n",
      "\n",
      "=== Encodage des variables catégorielles ===\n",
      "\n",
      "=== Normalisation des features numériques ===\n",
      "\n",
      "=== Assembly final ===\n",
      "\n",
      "=== Dataset prêt pour le modèle ===\n",
      "Nombre total de lignes: 172765\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                           |label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.7768282727817317,0.6632881255885652,1.0943234421372077,-0.3461632773000043,-0.874991396761714,1.33324370715565,0.3626133523238609,-0.7757599467342629,0.9360159184329607,0.6632881255885652,0.0,0.0,2.0,0.0,45.0,5.0,36.0]      |0    |\n",
      "|[0.7768282727817317,-2.5980694911208535,1.0670221427380402,-0.1958328458006766,-0.7330047632371504,1.33324370715565,-1.973383754277715,-0.7757599467342629,0.9360159184329607,-2.5980694911208535,1.0,0.0,2.0,0.0,133.0,9.0,36.0]  |1    |\n",
      "|[0.7768282727817317,-2.5855162491541663,1.053371746072637,-0.1206675642814435,-0.5910181865072288,1.33324370715565,-1.973383754277715,-0.7757599467342629,0.9360159184329607,-2.5855162491541663,3.0,0.0,2.0,0.0,133.0,9.0,36.0]   |0    |\n",
      "|[0.7768282727817317,0.007930989071698624,1.012503366506201,0.1043697696724924,-0.44903156718132564,1.33324370715565,-0.08744112159368994,-0.7757599467342629,0.9360159184329607,0.007930989071698624,0.0,0.0,2.0,2.0,10.0,3.0,36.0]|0    |\n",
      "|[0.7768282727817317,1.0749586509106361,0.9579012737762274,0.40503063312947235,-0.16505830013219855,1.33324370715565,0.7055120003428235,-0.7757599467342629,0.9360159184329607,1.0749586509106361,2.0,0.0,2.0,1.0,10.0,3.0,36.0]    |0    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Distribution de la variable cible ===\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|98977|\n",
      "|    0|73788|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# VOS FEATURES\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order State\", \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]\n",
    "\n",
    "# Charger les données\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*base_cols)\n",
    ")\n",
    "\n",
    "\n",
    "# 1. GESTION DES VALEURS MANQUANTES\n",
    "print(\"=== Vérification des valeurs manquantes ===\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "df_clean = df.dropna()\n",
    "print(f\"Lignes avant: {df.count()}, Lignes après: {df_clean.count()}\")\n",
    "\n",
    "\n",
    "# 2. ENCODAGE DES VARIABLES CATÉGORIELLES\n",
    "print(\"\\n=== Encodage des variables catégorielles ===\")\n",
    "indexers = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name, \n",
    "        outputCol=col_name + \"_indexed\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "pipeline_encoding = Pipeline(stages=indexers)\n",
    "model_encoding = pipeline_encoding.fit(df_clean)\n",
    "df_encoded = model_encoding.transform(df_clean)\n",
    "\n",
    "encoded_cols = [c + \"_indexed\" for c in feature_categorical_cols]\n",
    "\n",
    "\n",
    "# 3. NORMALISATION DES FEATURES NUMÉRIQUES\n",
    "print(\"\\n=== Normalisation des features numériques ===\")\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=feature_numeric_cols, \n",
    "    outputCol=\"numeric_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "df_assembled = assembler_num.transform(df_encoded)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\", \n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True, \n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "\n",
    "# 4. PRÉPARATION FINALE POUR LE MODÈLE\n",
    "print(\"\\n=== Assembly final ===\")\n",
    "all_feature_cols = [\"numeric_features_scaled\"] + encoded_cols\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=all_feature_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "df_final = assembler_final.transform(df_scaled)\n",
    "\n",
    "# Dataset prêt\n",
    "df_ready = df_final.select(\"features\", col(\"Late_delivery_risk\").alias(\"label\"))\n",
    "\n",
    "print(\"\\n=== Dataset prêt pour le modèle ===\")\n",
    "print(f\"Nombre total de lignes: {df_ready.count()}\")\n",
    "df_ready.show(5, truncate=False)\n",
    "\n",
    "# Distribution finale\n",
    "print(\"\\n=== Distribution de la variable cible ===\")\n",
    "df_ready.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ed009",
   "metadata": {},
   "source": [
    "## Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca30be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9455f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nettoyé: 172765 lignes\n",
      "\n",
      "=== Nombre de valeurs uniques par variable catégorielle ===\n",
      "Type: 4 valeurs uniques\n",
      "Shipping Mode: 4 valeurs uniques\n",
      "Market: 5 valeurs uniques\n",
      "Customer Segment: 3 valeurs uniques\n",
      "Order State: 1083 valeurs uniques\n",
      "Order Region: 23 valeurs uniques\n",
      "Category Name: 50 valeurs uniques\n",
      "\n",
      "Données d'entraînement: 138318 lignes\n",
      "Données de test: 34447 lignes\n",
      "\n",
      "============================================================\n",
      "MODÈLE : Random Forest\n",
      "============================================================\n",
      "⏳ Entraînement en cours...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1578.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 259.0 failed 1 times, most recent failure: Lost task 20.0 in stage 259.0 (TID 3162) (DESKTOP-7M3HR4V executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:653)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d26e4560.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:441)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d2617100.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD$$Lambda/0x00000177d1f32f70.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00000177d1e972a0.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:653)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d26e4560.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:441)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d2617100.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD$$Lambda/0x00000177d1f32f70.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00000177d1e972a0.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 253\u001b[39m\n\u001b[32m    250\u001b[39m resultats = []\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Random Forest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m resultats.append(\u001b[43mentrainer_et_evaluer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRandom Forest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_onehot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Gradient Boosting\u001b[39;00m\n\u001b[32m    256\u001b[39m resultats.append(entrainer_et_evaluer(gbt, \u001b[33m\"\u001b[39m\u001b[33mGradient Boosting Trees\u001b[39m\u001b[33m\"\u001b[39m, use_onehot=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mentrainer_et_evaluer\u001b[39m\u001b[34m(model, nom_modele, use_onehot)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Entraîner\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⏳ Entraînement en cours...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m model_trained = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Entraînement terminé !\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Prédictions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:138\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    136\u001b[39m     dataset = stage.transform(dataset)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     transformers.append(model)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < indexOfLastEstimator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1578.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 259.0 failed 1 times, most recent failure: Lost task 20.0 in stage 259.0 (TID 3162) (DESKTOP-7M3HR4V executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:653)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d26e4560.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:441)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d2617100.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD$$Lambda/0x00000177d1f32f70.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00000177d1e972a0.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:653)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d26e4560.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:441)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:649)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00000177d2617100.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\r\n\tat org.apache.spark.rdd.RDD$$Lambda/0x00000177d1f32f70.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00000177d1e972a0.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\r\n\t... 1 more\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Une connexion existante a dû être fermée par l’hôte distant\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Une connexion existante a dû être fermée par l’hôte distant\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3474\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3470\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# VOS FEATURES\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order State\", \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "cible = [\"Late_delivery_risk\"]\n",
    "\n",
    "# Charger les données\n",
    "base_cols = feature_numeric_cols + feature_categorical_cols + cible\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*base_cols)\n",
    ")\n",
    "\n",
    "# Nettoyer les données\n",
    "df_clean = df.dropna()\n",
    "print(f\"Dataset nettoyé: {df_clean.count()} lignes\")\n",
    "\n",
    "# Vérifier le nombre de valeurs uniques par colonne catégorielle\n",
    "print(\"\\n=== Nombre de valeurs uniques par variable catégorielle ===\")\n",
    "for col_name in feature_categorical_cols:\n",
    "    nb_unique = df_clean.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {nb_unique} valeurs uniques\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# ÉTAPES DE PRÉTRAITEMENT\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Encodage des variables catégorielles avec StringIndexer\n",
    "indexers = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name, \n",
    "        outputCol=col_name + \"_indexed\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "indexed_cols = [c + \"_indexed\" for c in feature_categorical_cols]\n",
    "\n",
    "# 2. OneHotEncoding pour les modèles qui en ont besoin (Neural Network)\n",
    "encoders = []\n",
    "for col_name in feature_categorical_cols:\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=col_name + \"_indexed\",\n",
    "        outputCol=col_name + \"_encoded\"\n",
    "    )\n",
    "    encoders.append(encoder)\n",
    "\n",
    "encoded_cols = [c + \"_encoded\" for c in feature_categorical_cols]\n",
    "\n",
    "# 3. Assembly des features numériques\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=feature_numeric_cols, \n",
    "    outputCol=\"numeric_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4. Normalisation\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\", \n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True, \n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# 5. Assembly final pour Random Forest et GBT (avec indexed)\n",
    "assembler_final_trees = VectorAssembler(\n",
    "    inputCols=[\"numeric_features_scaled\"] + indexed_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 6. Assembly final pour Neural Network (avec OneHot)\n",
    "assembler_final_mlp = VectorAssembler(\n",
    "    inputCols=[\"numeric_features_scaled\"] + encoded_cols, \n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# MODÈLES NON-LINÉAIRES\n",
    "# ====================================================================\n",
    "\n",
    "# MODÈLE 1 : RANDOM FOREST\n",
    "# SOLUTION : Augmenter maxBins pour gérer plus de catégories\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    maxBins=2000,  # ← AUGMENTÉ pour gérer Order State (1081 valeurs)\n",
    "    minInstancesPerNode=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# MODÈLE 2 : GRADIENT BOOSTING TREES\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    maxBins=2000,  # ← AUGMENTÉ également\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# MODÈLE 3 : NEURAL NETWORK\n",
    "# Pour MLP, on a besoin de compter les features après OneHotEncoding\n",
    "# On va le calculer dynamiquement après le preprocessing\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Late_delivery_risk\",\n",
    "    maxIter=100,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# SPLIT TRAIN/TEST\n",
    "# ====================================================================\n",
    "\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nDonnées d'entraînement: {train_data.count()} lignes\")\n",
    "print(f\"Données de test: {test_data.count()} lignes\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# FONCTION POUR ENTRAÎNER ET ÉVALUER UN MODÈLE\n",
    "# ====================================================================\n",
    "\n",
    "def entrainer_et_evaluer(model, nom_modele, use_onehot=False):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODÈLE : {nom_modele}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Choisir le bon assembler\n",
    "    if use_onehot:\n",
    "        assembler_final = assembler_final_mlp\n",
    "        stages = [*indexers, *encoders, assembler_num, scaler, assembler_final, model]\n",
    "    else:\n",
    "        assembler_final = assembler_final_trees\n",
    "        stages = [*indexers, assembler_num, scaler, assembler_final, model]\n",
    "    \n",
    "    # Créer le pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Entraîner\n",
    "    print(f\"⏳ Entraînement en cours...\")\n",
    "    model_trained = pipeline.fit(train_data)\n",
    "    print(f\"✅ Entraînement terminé !\")\n",
    "    \n",
    "    # Prédictions\n",
    "    predictions_train = model_trained.transform(train_data)\n",
    "    predictions_test = model_trained.transform(test_data)\n",
    "    \n",
    "    # Evaluateurs\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    \n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    \n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    auc_train = evaluator_auc.evaluate(predictions_train)\n",
    "    auc_test = evaluator_auc.evaluate(predictions_test)\n",
    "    \n",
    "    accuracy_train = evaluator_accuracy.evaluate(predictions_train)\n",
    "    accuracy_test = evaluator_accuracy.evaluate(predictions_test)\n",
    "    \n",
    "    f1_train = evaluator_f1.evaluate(predictions_train)\n",
    "    f1_test = evaluator_f1.evaluate(predictions_test)\n",
    "    \n",
    "    precision_test = evaluator_precision.evaluate(predictions_test)\n",
    "    recall_test = evaluator_recall.evaluate(predictions_test)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"\\n📊 RÉSULTATS - {nom_modele}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"TRAIN | AUC: {auc_train:.4f} | Accuracy: {accuracy_train:.4f} | F1: {f1_train:.4f}\")\n",
    "    print(f\"TEST  | AUC: {auc_test:.4f} | Accuracy: {accuracy_test:.4f} | F1: {f1_test:.4f}\")\n",
    "    print(f\"TEST  | Precision: {precision_test:.4f} | Recall: {recall_test:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    print(f\"\\n📈 Matrice de confusion - {nom_modele}\")\n",
    "    confusion_matrix = predictions_test.groupBy(\"Late_delivery_risk\", \"prediction\").count()\n",
    "    confusion_matrix.orderBy(\"Late_delivery_risk\", \"prediction\").show()\n",
    "    \n",
    "    return {\n",
    "        'modele': nom_modele,\n",
    "        'auc_test': auc_test,\n",
    "        'accuracy_test': accuracy_test,\n",
    "        'f1_test': f1_test,\n",
    "        'precision_test': precision_test,\n",
    "        'recall_test': recall_test,\n",
    "        'pipeline': model_trained\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# ENTRAÎNER ET COMPARER TOUS LES MODÈLES NON-LINÉAIRES\n",
    "# ====================================================================\n",
    "\n",
    "resultats = []\n",
    "\n",
    "# Random Forest\n",
    "resultats.append(entrainer_et_evaluer(rf, \"Random Forest\", use_onehot=False))\n",
    "\n",
    "# Gradient Boosting\n",
    "resultats.append(entrainer_et_evaluer(gbt, \"Gradient Boosting Trees\", use_onehot=False))\n",
    "\n",
    "# Neural Network (avec OneHot encoding)\n",
    "# D'abord, configurer les layers correctement\n",
    "# On va le faire après avoir compté les features\n",
    "print(\"\\n⏳ Configuration du Neural Network...\")\n",
    "\n",
    "# Créer un pipeline temporaire pour compter les features\n",
    "temp_pipeline = Pipeline(stages=[*indexers, *encoders, assembler_num, scaler, assembler_final_mlp])\n",
    "temp_model = temp_pipeline.fit(train_data.limit(100))  # Juste pour compter\n",
    "temp_data = temp_model.transform(train_data.limit(1))\n",
    "nb_features = temp_data.select(\"features\").first()[0].size\n",
    "\n",
    "print(f\"Nombre de features après preprocessing: {nb_features}\")\n",
    "\n",
    "# Configurer le MLP avec le bon nombre de features\n",
    "layers = [nb_features, 128, 64, 2]  # 2 classes\n",
    "mlp.setLayers(layers)\n",
    "\n",
    "resultats.append(entrainer_et_evaluer(mlp, \"Neural Network (MLP)\", use_onehot=True))\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# COMPARAISON FINALE\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARAISON DES MODÈLES NON-LINÉAIRES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Modèle':<30} {'AUC':<10} {'Accuracy':<10} {'F1-Score':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for r in resultats:\n",
    "    print(f\"{r['modele']:<30} {r['auc_test']:<10.4f} {r['accuracy_test']:<10.4f} {r['f1_test']:<10.4f} {r['precision_test']:<10.4f} {r['recall_test']:<10.4f}\")\n",
    "\n",
    "# Trouver le meilleur modèle\n",
    "meilleur = max(resultats, key=lambda x: x['f1_test'])\n",
    "print(f\"\\n🏆 MEILLEUR MODÈLE : {meilleur['modele']} (F1-Score: {meilleur['f1_test']:.4f})\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549a97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
