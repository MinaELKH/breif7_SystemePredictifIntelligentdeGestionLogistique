{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e4e6e-6204-4f65-9961-ae98830543e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test de recuperation de donnee \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Spark ---\n",
    "spark = SparkSession.builder.appName(\"count_test\").getOrCreate()\n",
    "\n",
    "# --- Colonnes ---\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "# --- Charger données ---\n",
    "df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*(feature_numeric_cols + feature_categorical_cols + [\"Late_delivery_risk\"]))\n",
    ").dropna()\n",
    "\n",
    "\n",
    "for c in feature_categorical_cols:\n",
    "    print(c, df.select(c).distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51c210ff-4778-4821-a369-50d809c814e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GBTClassifier =====\n",
      "Accuracy = 0.697117903930131\n",
      "F1-score = 0.6936591440259341\n",
      "Precision = 0.7463293681199681\n",
      "Recall = 0.697117903930131\n",
      "AUC = 0.7437718164261659\n",
      "\n",
      "===== RandomForest =====\n",
      "Accuracy = 0.6940902474526929\n",
      "F1-score = 0.6923440477551527\n",
      "Precision = 0.7323106600794665\n",
      "Recall = 0.6940902474526929\n",
      "AUC = 0.7284670397636691\n",
      "\n",
      "===== LogisticRegression =====\n",
      "Accuracy = 0.6939737991266376\n",
      "F1-score = 0.6922342061251481\n",
      "Precision = 0.7321398439841479\n",
      "Recall = 0.6939737991266376\n",
      "AUC = 0.7177239679216725\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# ---------- Winsorizer (cap outliers) ----------\n",
    "class Winsorizer(Transformer):\n",
    "    def __init__(self, inputCols=None, lower=0.01, upper=0.99):\n",
    "        super(Winsorizer, self).__init__()\n",
    "        self.inputCols = inputCols\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "    \n",
    "    def _transform(self, df):\n",
    "        for c in self.inputCols:\n",
    "            q = df.approxQuantile(c, [self.lower, self.upper], 0.01)\n",
    "            low, up = q\n",
    "\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                expr(\n",
    "                    f\"CASE \"\n",
    "                    f\"WHEN `{c}` < {low} THEN {low} \"\n",
    "                    f\"WHEN `{c}` > {up} THEN {up} \"\n",
    "                    f\"ELSE `{c}` END\"\n",
    "                )\n",
    "            )\n",
    "        return df\n",
    "\n",
    "\n",
    "# ---------- Spark ----------\n",
    "spark = SparkSession.builder.appName(\"ML_Models_Test\").getOrCreate()\n",
    "\n",
    "# ---------- Colonnes ----------\n",
    "feature_numeric_cols = [\n",
    "    \"Days for shipment (scheduled)\", \"Benefit per order\", \"Sales per customer\",\n",
    "    \"Order Item Discount\", \"Order Item Discount Rate\", \"Order Item Product Price\",\n",
    "    \"Order Item Profit Ratio\", \"Order Item Quantity\", \"Sales\", \"Order Profit Per Order\"\n",
    "]\n",
    "\n",
    "feature_categorical_cols = [\n",
    "    \"Type\", \"Shipping Mode\", \"Market\", \"Customer Segment\",\n",
    "    \"Order Region\", \"Category Name\"\n",
    "]\n",
    "\n",
    "# ---------- Data ----------\n",
    "df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True)\n",
    "    .csv(\"../data/DataCoSupplyChainDataset.csv\")\n",
    "    .filter(col(\"Delivery Status\") != \"Shipping canceled\")\n",
    "    .select(*(feature_numeric_cols + feature_categorical_cols + [\"Late_delivery_risk\"]))\n",
    ").dropna()\n",
    "\n",
    "# ---------- Prétraitement ----------\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\")\n",
    "    for c in feature_categorical_cols\n",
    "]\n",
    "\n",
    "winsor = Winsorizer(inputCols=feature_numeric_cols, lower=0.01, upper=0.99)\n",
    "\n",
    "assembler_num = VectorAssembler(inputCols=feature_numeric_cols, outputCol=\"num_features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=[\"num_features_scaled\"] + [c+\"_idx\" for c in feature_categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# ---------- Modèles ----------\n",
    "models = {\n",
    "    \"GBTClassifier\": GBTClassifier(featuresCol=\"features\", labelCol=\"Late_delivery_risk\",\n",
    "                                   maxIter=30, maxDepth=5, maxBins=2000),\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"Late_delivery_risk\",\n",
    "                                           numTrees=50, maxDepth=5, maxBins=2000),\n",
    "\n",
    "    \"LogisticRegression\": LogisticRegression(featuresCol=\"features\", labelCol=\"Late_delivery_risk\",\n",
    "                                             maxIter=50)\n",
    "}\n",
    "\n",
    "# ---------- Train/Test ----------\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ---------- Fonction évaluation ----------\n",
    "def evaluate_model(model_name, pipeline_model):\n",
    "    pred = pipeline_model.transform(test)\n",
    "\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "    evaluator_prec = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"Late_delivery_risk\", rawPredictionCol=\"rawPrediction\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    print(\"Accuracy =\", evaluator_acc.evaluate(pred))\n",
    "    print(\"F1-score =\", evaluator_f1.evaluate(pred))\n",
    "    print(\"Precision =\", evaluator_prec.evaluate(pred))\n",
    "    print(\"Recall =\", evaluator_recall.evaluate(pred))\n",
    "    print(\"AUC =\", evaluator_auc.evaluate(pred))\n",
    "\n",
    "# ---------- Entraînement + évaluation ----------\n",
    "# for name, classifier in models.items():\n",
    "#     pipeline = Pipeline(stages=indexers + [winsor, assembler_num, scaler, assembler_final, classifier])\n",
    "#     model = pipeline.fit(train)\n",
    "#     evaluate_model(name, model)\n",
    "\n",
    "for name, classifier in models.items():\n",
    "    pipeline = Pipeline(stages=indexers + [winsor, assembler_num, scaler, assembler_final, classifier])\n",
    "    model = pipeline.fit(train)\n",
    "    evaluate_model(name, model)\n",
    "\n",
    "    if name == \"LogisticRegression\":\n",
    "        best_model = model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea020d35-c748-4957-b1dd-1db2442b2b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du modèle dans ../models/logistic_regression_pipeline...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Pipeline write will fail on this pipeline because stage %s of type %s is not MLWritable', 'Winsorizer_2b4181ca1c24', <class '__main__.Winsorizer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/logistic_regression_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSauvegarde du modèle dans \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Pipeline sauvegardé avec succès!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ---------- Sauvegarde des métadonnées ----------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py:156\u001b[0m, in \u001b[0;36mMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshouldOverwrite:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handleOverwrite(path)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py:265\u001b[0m, in \u001b[0;36mPipelineModelWriter.saveImpl\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msaveImpl\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mstages\n\u001b[0;32m--> 265\u001b[0m     \u001b[43mPipelineSharedReadWrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidateStages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipelineStage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39msaveImpl(\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, cast(List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineStage\u001b[39m\u001b[38;5;124m\"\u001b[39m], stages), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc, path\n\u001b[1;32m    268\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py:392\u001b[0m, in \u001b[0;36mPipelineSharedReadWrite.validateStages\u001b[0;34m(stages)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m stages:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, MLWritable):\n\u001b[0;32m--> 392\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline write will fail on this pipeline \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause stage \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not MLWritable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    395\u001b[0m             stage\u001b[38;5;241m.\u001b[39muid,\n\u001b[1;32m    396\u001b[0m             \u001b[38;5;28mtype\u001b[39m(stage),\n\u001b[1;32m    397\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: ('Pipeline write will fail on this pipeline because stage %s of type %s is not MLWritable', 'Winsorizer_2b4181ca1c24', <class '__main__.Winsorizer'>)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------- Sauvegarde ----------\n",
    "model_path = \"../models/logistic_regression_pipeline\"\n",
    "print(f\"Sauvegarde du modèle dans {model_path}...\")\n",
    "\n",
    "best_model.write().overwrite().save(model_path)\n",
    "\n",
    "print(\"✓ Pipeline sauvegardé avec succès!\")\n",
    "\n",
    "# ---------- Sauvegarde des métadonnées ----------\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"model_type\": \"LogisticRegression\",\n",
    "    \"feature_numeric_cols\": feature_numeric_cols,\n",
    "    \"feature_categorical_cols\": feature_categorical_cols,\n",
    "    \"accuracy\": 0.6939737991266376,\n",
    "    \"f1_score\": 0.6922342061251481,\n",
    "    \"precision\": 0.7321398439841479,\n",
    "    \"recall\": 0.6939737991266376,\n",
    "    \"auc\": 0.7177266968937516\n",
    "}\n",
    "\n",
    "with open(\"../models/model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"✓ Métadonnées sauvegardées!\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
